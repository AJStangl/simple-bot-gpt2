{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers@main\n",
    "!pip install accelerate\n",
    "!pip install git+https://github.com/AJStangl/gpt-model-finetuning@master\n",
    "!pip install  simpletransformers==0.63.3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import gc\n",
    "import os\n",
    "import logging\n",
    "import pandas\n",
    "import torch\n",
    "import gc\n",
    "from shared_code.fine_tuning.datasets.reddit_dataset import RedditDataset\n",
    "from shared_code.fine_tuning.tensor_encoding.tensor_encoding import TokenizerAdapter\n",
    "from shared_code.fine_tuning.datasets.reddit_dataset import RedditDataset\n",
    "import pandas\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import logging\n",
    "import pandas\n",
    "from simpletransformers.language_modeling import LanguageModelingModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "use_head_model = False\n",
    "write_text_file = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = \"yuli-bot\"\n",
    "\n",
    "parent_directory = \"/content/drive/MyDrive/RawData\"\n",
    "\n",
    "model_output_dir = f\"{parent_directory}/{model_name}\"\n",
    "\n",
    "tokenizer_path = f\"{model_output_dir}\"\n",
    "\n",
    "training_data_path = f\"/content/drive/MyDrive/RawData/training.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "tokenizer_adapter = TokenizerAdapter(tokenizer)\n",
    "model = None\n",
    "if use_head_model:\n",
    "\ttokenizer.save_pretrained(model_output_dir)\n",
    "\tmodel = GPT2LMHeadModel.from_pretrained('gpt2-medium').cuda()\n",
    "\ttokenizer_adapter = TokenizerAdapter(tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def has_valid_line(input: str) -> bool:\n",
    "    black_list = [\"**NO SIGN**\", \"**Image Stats:**\", \"**INCOMPLETE MEAT TUBE**\", \"[removed]\", \"[deleted]\", 'Unfortunately, your post was removed for the following reason(s)']\n",
    "    for line in black_list:\n",
    "        if input.__contains__(line):\n",
    "            print(f\":: Line contains word {line}... Skipping\")\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pandas.read_csv(training_data_path)\n",
    "\n",
    "conversations = list(df['TrainingString'])\n",
    "\n",
    "valid_lines = []\n",
    "\n",
    "for conversation in conversations:\n",
    "\tif tokenizer_adapter.token_length_appropriate(conversation) and has_valid_line(conversation):\n",
    "\t\tvalid_lines.append(conversation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator = torch.Generator()\n",
    "\n",
    "generator.manual_seed(0)\n",
    "\n",
    "logging.info(f\":: Total Number Of Samples {len(valid_lines)}\")\n",
    "\n",
    "if use_head_model and model:\n",
    "\n",
    "\tmax_length = max([len(tokenizer.encode(prompt)) for prompt in valid_lines])\n",
    "\n",
    "\tmodel.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\tlogging.info(f\":: Max Length Of Sample {max_length}\")\n",
    "\n",
    "\tdataset = RedditDataset(valid_lines, tokenizer, max_length=max_length)\n",
    "\n",
    "\ttrain_size = int(0.9 * len(dataset))\n",
    "\n",
    "\ttrain_dataset, eval_dataset = random_split(dataset, [train_size, len(dataset) - train_size], generator=generator)\n",
    "\n",
    "else: # Do it the other way with a file\n",
    "\ttrain_size = int(0.9 * len(valid_lines))\n",
    "\n",
    "\ttrain_dataset, eval_dataset = random_split(list(valid_lines), [train_size, len(valid_lines) - train_size], generator=generator)\n",
    "\n",
    "\twith open(\"train.txt\", 'w', encoding=\"utf-8\") as train_out, open(\"eval.txt\", \"w\", encoding=\"utf-8\") as eval_out:\n",
    "\t\ttrain_out.writelines([repr(line)[1:-1] + \"<|endoftext|>\" + \"\\n\" for line in train_dataset])\n",
    "\t\teval_out.writelines([repr(line)[1:-1] + \"<|endoftext|>\" + \"\\n\" for line in eval_dataset])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if use_head_model:\n",
    "\ttraining_args = TrainingArguments(output_dir=model_output_dir)\n",
    "\ttraining_args.num_train_epochs = 5\n",
    "\ttraining_args.logging_steps = 100\n",
    "\ttraining_args.save_steps = 1000\n",
    "\ttraining_args.weight_decay = 0.05\n",
    "\ttraining_args.logging_dir = './logs'\n",
    "\ttraining_args.fp16 = True\n",
    "\ttraining_args.auto_find_batch_size = True\n",
    "\ttraining_args.gradient_accumulation_steps = 50\n",
    "\ttraining_args.learning_rate = 1e-4\n",
    "else:\n",
    "\ttraining_args = {\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"gradient_accumulation_steps\": 100,\n",
    "    \"dataset_type\": \"simple\",\n",
    "    \"sliding_window\": True,\n",
    "    \"max_seq_length\": 1024,\n",
    "\t\"mlm\": False, # has to be false for gpt-2\n",
    "    \"evaluate_during_training\": True,\n",
    "    \"use_cached_eval_features\": True,\n",
    "    \"evaluate_during_training_verbose\": True,\n",
    "    \"save_optimizer_and_scheduler\": False,\n",
    "    \"save_eval_checkpoints\": True,\n",
    "    \"save_model_every_epoch\": False,\n",
    "    \"save_steps\": -1,\n",
    "    \"train_batch_size\":3,\n",
    "    \"num_train_epochs\":12,\n",
    "    \"output_dir\": f\"{model_output_dir}/\",\n",
    "\t\"best_model_dir\": f\"{model_output_dir}/best_model\",\n",
    "}\n",
    "model = LanguageModelingModel(\"gpt2\", \"gpt2-medium\", args=training_args)\n",
    "model.train_model(train_file=\"train.txt\", eval_file=\"eval.txt\", args=training_args, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if use_head_model:\n",
    "\ttrainer: Trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset,\n",
    "\t\t\t\t\t\t\t   data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   'attention_mask': torch.stack([f[1] for f in data]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   'labels': torch.stack([f[0] for f in data])\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   })\n",
    "\ttrainer.train()\n",
    "\ttrainer.save_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "tag_match = f\"\\<\\|(.*)\\|\\>\"\n",
    "pattern = re.compile(tag_match)\n",
    "expected_prompt = \"I love these bizarro eras of weirdness in AI development after it becomes possible to do but before it's perfected. Image synthesis itself was in the same place for several years back in the mid to late 2010s.\"\n",
    "\n",
    "prompt = \"<|soss r/dalle2|><|sot|>Detailed scientific diagram depicting the anatomy of a tomato, full colour, realistic<|sost|>https://i.imgur.com/7adBOXn.jpg<|sor u/AsterJ|>It's going to be sad day when it learns to properly spell.  I feel like this era is a fleeting moment in AI history.  We must cherish it.<|eor|><|sor\"\n",
    "\n",
    "generated = tokenizer(f\"<|startoftext|> {prompt}\", return_tensors=\"pt\")\n",
    "\n",
    "sample_outputs = model.generate(inputs=generated.input_ids.cuda(),\n",
    "\t\t\t\t\t\t\t\tattention_mask=generated['attention_mask'].cuda(),\n",
    "                                do_sample=True,\n",
    "                                top_k=40,\n",
    "\t\t\t\t\t\t\t\tmax_length=1024,\n",
    "                                top_p=0.8,\n",
    "                                temperature=0.8,\n",
    "                                num_return_sequences=10,\n",
    "\t\t\t\t\t\t\t\trepetition_penalty=1.08,\n",
    "                                stop_token='<|endoftext|>')\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    result = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    print(\"{}: {}\".format(i, result.replace(prompt, \"\")))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
