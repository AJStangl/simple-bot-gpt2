{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install git+https: // github.com / huggingface / transformers @ main > / dev / null\n",
    "!pip install accelerate > / dev / null\n",
    "!pip install  simpletransformers == 0.63.3 > / dev / null"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "\n",
    "import pandas\n",
    "import torch\n",
    "from simpletransformers.language_modeling import LanguageModelingModel\n",
    "from torch.utils.data import random_split\n",
    "from transformers import GPT2Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = \"foo-bot-gpt2\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parent_directory = \"/path/to/parent/dir/for/whatever\"\n",
    "\n",
    "data_dir = f\"{parent_directory}/data\"\n",
    "\n",
    "model_output_dir = f\"{parent_directory}/{model_name}\"\n",
    "\n",
    "tokenizer_path = f\"{model_output_dir}\"\n",
    "\n",
    "training_data_path = f\"{data_dir}/{model_name}-training.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def has_valid_line(input: str) -> bool:\n",
    "\tblack_list = [\"**NO SIGN**\", \"**Image Stats:**\", \"**INCOMPLETE MEAT TUBE**\", \"[removed]\", \"[deleted]\",\n",
    "\t\t\t\t  'Unfortunately, your post was removed for the following reason(s)']\n",
    "\tfor line in black_list:\n",
    "\t\tif input.__contains__(line):\n",
    "\t\t\tprint(f\":: Line contains word {line}... Skipping\")\n",
    "\t\t\treturn False\n",
    "\t\telse:\n",
    "\t\t\treturn True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "\n",
    "\n",
    "def token_length_appropriate(prompt) -> bool:\n",
    "\t\"\"\"\n",
    "    Ensures that the total number of encoded tokens is within acceptable limits.\n",
    "    :param tokenizer: An instance of the tokenizer being used.\n",
    "    :param prompt: UTF-8 Text that is assumed to have been processed.\n",
    "    :return: True if acceptable.\n",
    "    \"\"\"\n",
    "\ttokens = tokenizer.tokenize(prompt)\n",
    "\tif len(tokens) > 1024:\n",
    "\t\tprint(f\":: Tokens for model input is > {1024}. Skipping input\")\n",
    "\t\treturn False\n",
    "\telse:\n",
    "\t\treturn True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pandas.read_csv(training_data_path)\n",
    "\n",
    "conversations = list(df['TrainingString'])\n",
    "\n",
    "valid_lines = []\n",
    "for conversation in conversations:\n",
    "    # if has_valid_line(conversation):\n",
    "    if token_length_appropriate(conversation) and has_valid_line(conversation):\n",
    "        valid_lines.append(conversation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator = torch.Generator()\n",
    "\n",
    "generator.manual_seed(0)\n",
    "\n",
    "print(f\":: Total Number Of Samples {len(valid_lines)}\")\n",
    "\n",
    "train_size = int(0.9 * len(valid_lines))\n",
    "\n",
    "train_dataset_file, eval_dataset_file = random_split(list(valid_lines), [train_size, len(valid_lines) - train_size], generator=generator)\n",
    "\n",
    "with open(\"train.txt\", 'w', encoding=\"utf-8\") as train_out, open(\"eval.txt\", \"w\", encoding=\"utf-8\") as eval_out:\n",
    "    # for line in train_dataset_file:\n",
    "        train_out.writelines([repr(line)[1:-1] + \"<|endoftext|>\" + \"\\n\" for line in train_dataset_file])\n",
    "\n",
    "    # for line in eval_dataset_file:\n",
    "        eval_out.writelines([repr(line)[1:-1] + \"<|endoftext|>\" + \"\\n\" for line in eval_dataset_file])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "args = {\n",
    "\t\"overwrite_output_dir\": True,\n",
    "\t\"learning_rate\": 1e-4,\n",
    "\t\"gradient_accumulation_steps\": 100,\n",
    "\t\"dataset_type\": \"simple\",\n",
    "\t\"sliding_window\": True,\n",
    "\t\"max_seq_length\": 1024,\n",
    "\t\"mlm\": False,  # has to be false for gpt-2\n",
    "\t\"evaluate_during_training\": True,\n",
    "\t\"use_cached_eval_features\": True,\n",
    "\t\"evaluate_during_training_verbose\": True,\n",
    "\t\"save_optimizer_and_scheduler\": False,\n",
    "\t\"save_eval_checkpoints\": False,\n",
    "\t\"save_model_every_epoch\": True,\n",
    "\t\"save_steps\": -1,\n",
    "\t\"train_batch_size\": 3,\n",
    "\t\"num_train_epochs\": 12,\n",
    "\t\"output_dir\": f\"{model_output_dir}/\",\n",
    "\t\"best_model_dir\": f\"{model_output_dir}/best_model\"\n",
    "}\n",
    "model = LanguageModelingModel(\"gpt2\", \"gpt2-medium\", args=args)\n",
    "model.train_model(train_file=\"train.txt\", eval_file=\"eval.txt\", args=args, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def capture_tag(test_string: str, expected_tag: str):\n",
    "\tregex = r\"\\<\\|(.*)\\|\\>\"\n",
    "\n",
    "\tmatches = re.finditer(regex, test_string, re.MULTILINE)\n",
    "\n",
    "\tfor matchNum, match in enumerate(matches, start=1):\n",
    "\n",
    "\t\tprint(\"Match {matchNum} was found at {start}-{end}: {match}\".format(matchNum=matchNum, start=match.start(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tend=match.end(), match=match.group()))\n",
    "\n",
    "\t\tif match.group() == expected_tag:\n",
    "\t\t\treturn_string = test_string.replace(match.group(), \"\")\n",
    "\t\t\treturn return_string\n",
    "\n",
    "\t\tfor groupNum in range(0, len(match.groups())):\n",
    "\t\t\tgroupNum = groupNum + 1\n",
    "\n",
    "\t\t\tprint(\"Group {groupNum} found at {start}-{end}: {group}\".format(groupNum=groupNum,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstart=match.start(groupNum),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tend=match.end(groupNum),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tgroup=match.group(groupNum)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from simpletransformers.language_generation import LanguageGenerationModel\n",
    "\n",
    "text_model_generator = LanguageGenerationModel(\"gpt2\", f\"{model_output_dir}/best_model\", args={\n",
    "\t'max_length': 1000,\n",
    "\t'num_return_sequences': 1,\n",
    "\t'repetition_penalty': 1.01,\n",
    "\t'stop_token': '<|endoftext|>',\n",
    "\t'temperature': 0.8,\n",
    "\t'top_k': 40,\n",
    "})\n",
    "\n",
    "print(\n",
    "\t\"It's going to be sad day when it learns to properly spell.  I feel like this era is a fleeting moment in AI history.  We must cherish it.\")\n",
    "prompt = \"<|soss r/dalle2|><|sot|>Detailed scientific diagram depicting the anatomy of a tomato, full colour, realistic<|sost|>https://i.imgur.com/7adBOXn.jpg<|sor u/AsterJ|>It's going to be sad day when it learns to properly spell.  I feel like this era is a fleeting moment in AI history.  We must cherish it.<|eor|><|sor|>\"\n",
    "\n",
    "import re\n",
    "\n",
    "regex = r\"\\<\\|(.*)\\|\\>\"\n",
    "\n",
    "reply = None\n",
    "refresh_args = {\n",
    "\t'max_length': 1000,\n",
    "\t'num_return_sequences': 1,\n",
    "\t'repetition_penalty': 1.01,\n",
    "\t'stop_token': '<|endoftext|>',\n",
    "\t'temperature': 0.8,\n",
    "\t'top_k': 40,\n",
    "}\n",
    "while reply is None:\n",
    "\tfor text in text_model_generator.generate(prompt=prompt, args=refresh_args, verbose=True):\n",
    "\t\tfoo = text.replace(prompt, \"\\n\")\n",
    "\t\tresult = capture_tag(foo, \"<|eor|>\")\n",
    "\t\tif result != None:\n",
    "\t\t\treply = result\n",
    "\t\t\tbreak\n",
    "print(reply)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
